{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "from pymongo import MongoClient\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "client = MongoClient('localhost', 27017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_dataframe(tab):\n",
    "\n",
    "    dataSet = pd.DataFrame()    \n",
    "    dataSet[\"id\"] = [tweet['id'] for tweet in tab.find()]\n",
    "    dataSet[\"text\"] = [tweet['text'] for tweet in tab.find()]\n",
    "    dataSet[\"created_at\"] = [tweet['created_at'] for tweet in tab.find()]\n",
    "    dataSet[\"favorite_count\"] = [tweet['favorite_count'] for tweet in tab.find()]\n",
    "    #dataSet[\"retweet_count\"] = [tweet['retweeted_status']['retweet_count'] tweet in tab.find()]\n",
    "    dataSet[\"source\"] = [tweet['source'] for tweet in tab.find()]\n",
    "    dataSet[\"user_id\"] = [tweet['user']['id'] for tweet in tab.find()]\n",
    "    dataSet[\"user_screen_name\"] = [tweet['user']['screen_name'] for tweet in tab.find()]\n",
    "    dataSet[\"user_name\"] = [tweet['user']['name'] for tweet in tab.find()]\n",
    "    dataSet[\"user_created_at\"] = [tweet['user']['created_at'] for tweet in tab.find()]\n",
    "    dataSet[\"user_description\"] = [tweet['user']['description'] for tweet in tab.find()]\n",
    "    dataSet[\"user_followers_count\"] = [tweet['user']['followers_count'] for tweet in tab.find()]\n",
    "    dataSet[\"user_friends_count\"] = [tweet['user']['friends_count'] for tweet in tab.find()]\n",
    "    dataSet[\"user_location\"] = [tweet['user']['location'] for tweet in tab.find()]\n",
    "    dataSet[\"user_time_zone\"] = [tweet['user']['time_zone'] for tweet in tab.find()]\n",
    "\n",
    "    return dataSet.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_category_sent(score):\n",
    "    if score > 0 :\n",
    "        return \"Pos\"\n",
    "    elif score < 0:\n",
    "        return \"Neg\"\n",
    "    else:\n",
    "        return \"Neu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_week(date_time):\n",
    "    \n",
    "    day7 = timedelta(days=7)\n",
    "    date_slected = datetime.date(2016,1, 9)\n",
    "    all_date =[date_slected]\n",
    "    for i in range(51):\n",
    "        date_slected = date_slected + day7\n",
    "        all_date.append(date_slected)\n",
    "\n",
    "    week_firstDate = pd.DataFrame()\n",
    "    week_firstDate['firstData'] = all_date\n",
    "    week_firstDate['week'] = range(1,53)\n",
    "    \n",
    "    for i in range(52):\n",
    "        if date_time.date() <= week_firstDate['firstData'][i]:\n",
    "            return week_firstDate['week'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def info_twitter_song_check(tab):\n",
    "\n",
    "    df= get_dataframe(tab)\n",
    "    df['sent_score'] = df['text'].map(lambda x : TextBlob(x).sentiment.polarity)\n",
    "    df['sent_category'] = df['sent_score'].map(get_category_sent)\n",
    "    df['pos'] = df['sent_category'].map(lambda x: 1 if x == 'Pos' else 0)\n",
    "    df['neg'] = df['sent_category'].map(lambda x: 1 if x == 'Neg' else 0)\n",
    "    df['neu'] = df['sent_category'].map(lambda x: 1 if x == 'Neu' else 0)\n",
    "    df['created_at_time'] = df['created_at'].map(lambda x : datetime.datetime.strptime(x, \"%a %b %d %H:%M:%S +0000 %Y\"))\n",
    "    df['created_at_time_min'] = df['created_at_time']\n",
    "    df['created_at_time_max'] = df['created_at_time']\n",
    "    df['created_at_time_date'] = df['created_at_time'].map(lambda x : x.date())\n",
    "    df['week_from_one'] = df['created_at_time'].map(find_week)\n",
    "    df['count'] = 1\n",
    "    \n",
    "    return df[['created_at_time','week_from_one']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db = client['song3_database']\n",
    "list_input = get_billboard(\"../data/billboard.csv\")\n",
    "current_billboard = get_billboard(\"../data/billboard2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "song_id = 1\n",
    "table_name = \"test_01_\" + str(song_id)\n",
    "tab = db[table_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at_time</th>\n",
       "      <th>week_from_one</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-04-18 15:38:11</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-04-18 14:59:39</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-04-18 13:55:11</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-04-18 13:45:27</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-04-18 13:44:59</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2016-04-18 13:00:00</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2016-04-18 11:55:32</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2016-04-18 11:30:27</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2016-04-18 10:15:30</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2016-04-18 10:03:53</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2016-04-18 09:12:08</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2016-04-18 08:14:57</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2016-04-18 06:50:08</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2016-04-18 06:33:50</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2016-04-18 05:34:59</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2016-04-18 04:16:02</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2016-04-18 04:10:05</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2016-04-18 03:35:15</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2016-04-18 03:35:00</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2016-04-18 02:02:42</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2016-04-18 00:35:22</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2016-04-18 00:34:44</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2016-04-17 23:53:40</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2016-04-17 22:30:28</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2016-04-17 22:27:00</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2016-04-17 22:21:33</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2016-04-17 21:24:06</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2016-04-17 20:51:41</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2016-04-17 20:42:13</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2016-04-17 19:54:31</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>2016-04-09 07:10:29</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>2016-04-09 04:54:15</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>2016-04-17 17:16:07</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>2016-04-17 02:03:46</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>2016-04-15 04:45:47</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>2016-04-14 19:47:07</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>2016-04-14 18:57:09</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515</th>\n",
       "      <td>2016-04-13 19:56:01</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531</th>\n",
       "      <td>2016-04-13 04:40:32</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546</th>\n",
       "      <td>2016-04-12 18:58:44</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551</th>\n",
       "      <td>2016-04-12 18:15:07</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570</th>\n",
       "      <td>2016-04-12 02:49:51</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>589</th>\n",
       "      <td>2016-04-11 18:23:01</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>590</th>\n",
       "      <td>2016-04-11 17:21:55</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>602</th>\n",
       "      <td>2016-04-11 10:18:38</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>606</th>\n",
       "      <td>2016-04-11 09:09:31</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>607</th>\n",
       "      <td>2016-04-11 09:09:29</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608</th>\n",
       "      <td>2016-04-11 09:09:26</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>609</th>\n",
       "      <td>2016-04-11 09:09:20</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>610</th>\n",
       "      <td>2016-04-11 09:09:17</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613</th>\n",
       "      <td>2016-04-11 06:15:42</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>618</th>\n",
       "      <td>2016-04-11 04:37:04</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>631</th>\n",
       "      <td>2016-04-10 22:20:12</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>646</th>\n",
       "      <td>2016-04-10 17:19:01</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>651</th>\n",
       "      <td>2016-04-10 13:48:54</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>659</th>\n",
       "      <td>2016-04-10 11:41:37</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>662</th>\n",
       "      <td>2016-04-10 08:43:51</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>663</th>\n",
       "      <td>2016-04-10 08:39:17</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>664</th>\n",
       "      <td>2016-04-10 08:29:15</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>682</th>\n",
       "      <td>2016-04-09 20:24:20</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>375 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        created_at_time  week_from_one\n",
       "0   2016-04-18 15:38:11             16\n",
       "1   2016-04-18 14:59:39             16\n",
       "2   2016-04-18 13:55:11             16\n",
       "3   2016-04-18 13:45:27             16\n",
       "4   2016-04-18 13:44:59             16\n",
       "5   2016-04-18 13:00:00             16\n",
       "6   2016-04-18 11:55:32             16\n",
       "7   2016-04-18 11:30:27             16\n",
       "8   2016-04-18 10:15:30             16\n",
       "9   2016-04-18 10:03:53             16\n",
       "10  2016-04-18 09:12:08             16\n",
       "11  2016-04-18 08:14:57             16\n",
       "12  2016-04-18 06:50:08             16\n",
       "13  2016-04-18 06:33:50             16\n",
       "14  2016-04-18 05:34:59             16\n",
       "15  2016-04-18 04:16:02             16\n",
       "16  2016-04-18 04:10:05             16\n",
       "17  2016-04-18 03:35:15             16\n",
       "18  2016-04-18 03:35:00             16\n",
       "19  2016-04-18 02:02:42             16\n",
       "20  2016-04-18 00:35:22             16\n",
       "21  2016-04-18 00:34:44             16\n",
       "22  2016-04-17 23:53:40             16\n",
       "23  2016-04-17 22:30:28             16\n",
       "24  2016-04-17 22:27:00             16\n",
       "25  2016-04-17 22:21:33             16\n",
       "26  2016-04-17 21:24:06             16\n",
       "27  2016-04-17 20:51:41             16\n",
       "28  2016-04-17 20:42:13             16\n",
       "29  2016-04-17 19:54:31             16\n",
       "..                  ...            ...\n",
       "345 2016-04-09 07:10:29             14\n",
       "346 2016-04-09 04:54:15             14\n",
       "381 2016-04-17 17:16:07             16\n",
       "401 2016-04-17 02:03:46             16\n",
       "465 2016-04-15 04:45:47             15\n",
       "479 2016-04-14 19:47:07             15\n",
       "482 2016-04-14 18:57:09             15\n",
       "515 2016-04-13 19:56:01             15\n",
       "531 2016-04-13 04:40:32             15\n",
       "546 2016-04-12 18:58:44             15\n",
       "551 2016-04-12 18:15:07             15\n",
       "570 2016-04-12 02:49:51             15\n",
       "589 2016-04-11 18:23:01             15\n",
       "590 2016-04-11 17:21:55             15\n",
       "602 2016-04-11 10:18:38             15\n",
       "606 2016-04-11 09:09:31             15\n",
       "607 2016-04-11 09:09:29             15\n",
       "608 2016-04-11 09:09:26             15\n",
       "609 2016-04-11 09:09:20             15\n",
       "610 2016-04-11 09:09:17             15\n",
       "613 2016-04-11 06:15:42             15\n",
       "618 2016-04-11 04:37:04             15\n",
       "631 2016-04-10 22:20:12             15\n",
       "646 2016-04-10 17:19:01             15\n",
       "651 2016-04-10 13:48:54             15\n",
       "659 2016-04-10 11:41:37             15\n",
       "662 2016-04-10 08:43:51             15\n",
       "663 2016-04-10 08:39:17             15\n",
       "664 2016-04-10 08:29:15             15\n",
       "682 2016-04-09 20:24:20             14\n",
       "\n",
       "[375 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info_twitter_song_check(tab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def info_twitter_song(tab):\n",
    "\n",
    "    df= get_dataframe(tab)\n",
    "    df['sent_score'] = df['text'].map(lambda x : TextBlob(x).sentiment.polarity)\n",
    "    df['sent_category'] = df['sent_score'].map(get_category_sent)\n",
    "    df['pos'] = df['sent_category'].map(lambda x: 1 if x == 'Pos' else 0)\n",
    "    df['neg'] = df['sent_category'].map(lambda x: 1 if x == 'Neg' else 0)\n",
    "    df['neu'] = df['sent_category'].map(lambda x: 1 if x == 'Neu' else 0)\n",
    "    df['created_at_time'] = df['created_at'].map(lambda x : datetime.datetime.strptime(x, \"%a %b %d %H:%M:%S +0000 %Y\"))\n",
    "    df['created_at_time_min'] = df['created_at_time']\n",
    "    df['created_at_time_max'] = df['created_at_time']\n",
    "    df['created_at_time_date'] = df['created_at_time'].map(lambda x : x.date())\n",
    "    df['week_from_one'] = df['created_at_time'].map(find_week)\n",
    "    df['count'] = 1\n",
    "    #df['source_short'] = df['source'].map(lambda x : str(x.split('>')[1].split('<')[0]))\n",
    "\n",
    "    col_list = ['week_from_one']\n",
    "    #agg_dic = {'favorite_count':sum, 'count':sum, 'pos':sum, 'neg':sum, 'neu':sum, 'created_at_time': [min, max]}\n",
    "    agg_dic = {'favorite_count':sum, 'count':sum, 'pos':sum, 'neg':sum, 'neu':sum,'created_at_time_min': min,\\\n",
    "               'created_at_time_max': max}\n",
    "    grouped = df.groupby(col_list).agg(agg_dic)\n",
    "    grouped = grouped.reset_index()\n",
    "\n",
    "    grouped['neg_rate']=1.0*grouped['neg']/grouped['count']\n",
    "    grouped['pos_rate']=1.0*grouped['pos']/grouped['count']\n",
    "    grouped['neu_rate']=1.0*grouped['neu']/grouped['count']\n",
    "    grouped['ratio_pos_neg']=1.0*(grouped['pos']+1)/(grouped['neg'] +1)\n",
    "    grouped['favorite_rate']=1.0*grouped['favorite_count']/grouped['count']\n",
    "\n",
    "    #selected_col = ['week_from_one','count','pos_rate','neg_rate', 'neu_rate','ratio_pos_neg','favorite_rate']\n",
    "    #slected = grouped[selected_col]\n",
    "    slected = grouped\n",
    "    return slected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_billboard(billboard_filename_path):\n",
    "\n",
    "    df2 = pd.read_csv(billboard_filename_path, sep='|', names=['last_date','rank','song','long_artist'])\n",
    "    df2['last_date_time'] = df2['last_date'].map(lambda x : datetime.datetime.strptime(x, \"%Y-%m-%d\"))\n",
    "    df2['week_from_one'] = df2['last_date_time'].map(find_week)\n",
    "    df2[\"artist\"] = df2[\"long_artist\"].map(lambda x : x.split('featuring')[0])\n",
    "    df2[\"ID\"] = df2[\"song\"] + \"%\" +  df2[\"artist\"]\n",
    "    \n",
    "    df= df2.pivot('ID', 'week_from_one', 'rank')\n",
    "    df[\"ID\"] = df.index\n",
    "    df[\"song\"] = df[\"ID\"].map(lambda x : x.split('%')[0])\n",
    "    df[\"artist\"] = df[\"ID\"].map(lambda x : x.split('%')[1])\n",
    "    df.fillna(101,inplace=True)\n",
    "    df.index = range(1,len(df)+1)\n",
    "    df[\"IDN\"] = df.index\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_date(week_from_one):\n",
    "    \n",
    "    day7 = timedelta(days=7)\n",
    "    day6 = timedelta(days=6)\n",
    "    date_slected = datetime.date(2016,1,3)\n",
    "    all_date =[date_slected]\n",
    "    all_date_last = [date_slected + day6]\n",
    "    for i in range(51):\n",
    "        date_slected = date_slected + day7\n",
    "        all_date.append(date_slected)\n",
    "        all_date_last.append(date_slected +day6)\n",
    "        \n",
    "        \n",
    "\n",
    "    week_Date = pd.DataFrame()\n",
    "    week_Date['firstData'] = all_date\n",
    "    week_Date['lastData'] = all_date_last\n",
    "    week_Date['week'] = range(1,53)\n",
    "    index_date = week_from_one - 1\n",
    "    \n",
    "    return str(all_date[index_date]) + ' ~ ' + str(all_date_last[index_date])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def list_no_twitter(list_id, db):\n",
    "    list_count = []\n",
    "    for song_id in list_id:\n",
    "        table_name = \"test_01_\" + str(song_id)\n",
    "        tab = db[table_name]\n",
    "        list_count.append(tab.find().count())\n",
    "        np\n",
    "    \n",
    "    return list(np.array(list_id)[np.array(list_count) == 0])\n",
    "\n",
    "db = client['song3_database']\n",
    "list_input = get_billboard(\"../data/billboard.csv\")\n",
    "list_id = range(len(list_input))    \n",
    "\n",
    "#list_no_twitter(list_id, db)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom collections import Counter\\n\\nlist_song_df_data_csv = Counter(df_data_csv['Song_IDN']-1).keys()\\n\\nNo_list_song_df_data_csv = list(set(range(146)) - set(list_song_df_data_csv))\\nNO_data_twitter = list(np.where (np.array(list_count) == 0)[0])\\n\\nprint len(No_list_song_df_data_csv), len(NO_data_twitter)\\nprint set(No_list_song_df_data_csv) - set(NO_data_twitter)\\nprint set(NO_data_twitter) - set(No_list_song_df_data_csv)\\n\\nprint len(list_song_df_data_csv), len(list_input)\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from collections import Counter\n",
    "\n",
    "list_song_df_data_csv = Counter(df_data_csv['Song_IDN']-1).keys()\n",
    "\n",
    "No_list_song_df_data_csv = list(set(range(146)) - set(list_song_df_data_csv))\n",
    "NO_data_twitter = list(np.where (np.array(list_count) == 0)[0])\n",
    "\n",
    "print len(No_list_song_df_data_csv), len(NO_data_twitter)\n",
    "print set(No_list_song_df_data_csv) - set(NO_data_twitter)\n",
    "print set(NO_data_twitter) - set(No_list_song_df_data_csv)\n",
    "\n",
    "print len(list_song_df_data_csv), len(list_input)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def twitter_info_table(song_id, db, list_input, current_billboard, week_all):\n",
    "\n",
    "    table_name = \"test_01_\" + str(song_id)\n",
    "    tab = db[table_name]\n",
    "    \n",
    "    Input = [ [\"#\"+str(id.split('%')[0]), str(id.split('%')[1])]  for id in list_input[\"ID\"]]\n",
    "    max_week = max([x for x in list(current_billboard.columns) if isinstance(x, int)])\n",
    "    current_billboard[max_week+1] = 0\n",
    "    Song_history = current_billboard[current_billboard['ID']==list_input['ID'].values[song_id]]\n",
    "\n",
    "    twit = info_twitter_song(tab)\n",
    "    twit['diff_time'] = twit['created_at_time_max'] - twit['created_at_time_min']\n",
    "    twit['diff_hour'] = twit['diff_time'].astype(pd.Timedelta).map(lambda x : float(x.seconds)/3600)\n",
    "    twit['diff_hour_adj'] = twit['diff_hour'].map(lambda x: 24*7 if x == 0 else x )\n",
    "    twit['twitter_per_hour'] = 1.0*twit['count']/twit['diff_hour_adj']\n",
    "    \n",
    "    \n",
    "    col = ['week_from_one', 'twitter_per_hour','pos_rate', 'neg_rate','ratio_pos_neg' ,'favorite_rate']\n",
    "    twitter_table = twit[col]\n",
    "    \n",
    "    diff = set(week_all) - set(list(twitter_table['week_from_one'].values))\n",
    "    \n",
    "    if len(diff) > 0:\n",
    "        for i in diff:\n",
    "            add_one = twitter_table.copy().iloc[0:1,]\n",
    "            add_one['week_from_one'] = i\n",
    "            add_one['twitter_per_hour'] = 1.0/(24*7)\n",
    "            add_one['pos_rate'] = 0\n",
    "            add_one['neg_rate'] = 0\n",
    "            add_one['ratio_pos_neg'] = 1\n",
    "            add_one['favorite_rate'] = 0\n",
    "            twitter_table = pd.concat([add_one, twitter_table])\n",
    "            \n",
    "    twitter_table['Billboard_rank'] = [int(Song_history[week]) for week in twitter_table['week_from_one']]\n",
    "    twitter_table['date_period'] = twitter_table['week_from_one'].map(find_date)\n",
    "    #twitter_table['Billboard_rank_text'] = twitter_table['Billboard_rank'].map(lambda x : x if x <= 100 else \"Not on Billboard\")\n",
    "    twitter_table['Song_ID'] = list_input[\"ID\"].values[song_id]\n",
    "    twitter_table['Song_IDN'] = song_id+1\n",
    "    \n",
    "    \n",
    "    return Song_history, twitter_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def twitter_info_table_no_twitter(song_id, db, list_input, current_billboard, week_all):\n",
    "\n",
    "\n",
    "    Input = [ [\"#\"+str(id.split('%')[0]), str(id.split('%')[1])]  for id in list_input[\"ID\"]]\n",
    "    max_week = max([x for x in list(current_billboard.columns) if isinstance(x, int)])\n",
    "    current_billboard[max_week+1] = 0\n",
    "    \n",
    "    Song_history = current_billboard[current_billboard['ID']==list_input['ID'].values[song_id]]\n",
    "\n",
    "    \n",
    "    twitter_table = pd.DataFrame()\n",
    "    twitter_table['week_from_one'] = week_all\n",
    "    twitter_table['twitter_per_hour'] = 1.0/(24*7)\n",
    "    twitter_table['pos_rate'] = 0\n",
    "    twitter_table['neg_rate'] = 0\n",
    "    twitter_table['ratio_pos_neg'] = 1\n",
    "    twitter_table['favorite_rate'] = 0\n",
    "            \n",
    "    twitter_table['Billboard_rank'] = [int(Song_history[week]) for week in twitter_table['week_from_one']]\n",
    "    twitter_table['date_period'] = twitter_table['week_from_one'].map(find_date)\n",
    "    #twitter_table['Billboard_rank_text'] = twitter_table['Billboard_rank'].map(lambda x : x if x <= 100 else \"Not on Billboard\")\n",
    "    twitter_table['Song_ID'] = list_input[\"ID\"].values[song_id]\n",
    "    twitter_table['Song_IDN'] = song_id+1\n",
    "    \n",
    "    return Song_history, twitter_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_DataFrame_for_song(song_id, db, list_input, current_billboard, week_all):\n",
    "    \n",
    "    NoTwitter = list_no_twitter(range(len(list_input)), db)\n",
    "    \n",
    "    if song_id in NoTwitter:\n",
    "        Song_history, twitter_table = twitter_info_table_no_twitter(song_id, db, list_input, current_billboard, week_all)\n",
    "    else:\n",
    "        Song_history, twitter_table = twitter_info_table(song_id, db, list_input, current_billboard, week_all)\n",
    "    \n",
    "    df_song = twitter_table\n",
    "    df_song = df_song.rename(columns = {'week_from_one':'week'})\n",
    "    df_song = df_song.rename(columns = {'Billboard_rank':'current_rank'})\n",
    "    df_song['past_rank_1'] = df_song['week'].map(lambda x : int(Song_history[x-1]))\n",
    "    df_song['past_rank_2'] = df_song['week'].map(lambda x : int(Song_history[x-2]))\n",
    "    df_song['past_rank_3'] = df_song['week'].map(lambda x : int(Song_history[x-3]))\n",
    "    df_song['past_rank_4'] = df_song['week'].map(lambda x : int(Song_history[x-4]))\n",
    "    df_song['next_rank'] = df_song['week'].map(lambda x : int(Song_history[x+1]))                                     \n",
    "    \n",
    "    \n",
    "    col =['date_period','week','current_rank','count','twitter_per_hour','pos_rate',\\\n",
    "          'neg_rate','ratio_pos_neg','favorite_rate','Song_ID','Song_IDN',\\\n",
    "          'past_rank_1','past_rank_2','past_rank_3','past_rank_4','past_rank_4','next_rank']\n",
    "    \n",
    "    return df_song\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def merge_dataframe(list_id, db, list_input, current_billboard, file_name, week_all=[]):\n",
    "    \n",
    "    print \"We are getting data frame for id = {}\".format(list_id[0])\n",
    "    df = get_DataFrame_for_song(list_id[0], db, list_input, current_billboard, week_all)    \n",
    "    print \"The length of dataFrame is {}.\".format(len(df))\n",
    "    print \" \"\n",
    "    \n",
    "    if len(list_id) == 1:\n",
    "        print \"We're done\"\n",
    "        return df\n",
    "    \n",
    "    \n",
    "    for i in range(1, len(list_id)):\n",
    "        print \"We completed {} of the total work and now are merging the data frame with id = {}\".format\\\n",
    "        (1.0*i/len(list_id), list_id[i])\n",
    "        \n",
    "        add_df = get_DataFrame_for_song(list_id[i], db, list_input, current_billboard, week_all)\n",
    "        df = pd.concat([df, add_df])\n",
    "        print \"The length of dataFrame is {}.\".format(len(df))\n",
    "        print \" \"\n",
    "    \n",
    "    df.to_csv(file_name, sep=',', encoding='utf-8')\n",
    "    print \"We're done\"\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db = client['song3_database']\n",
    "list_input = get_billboard(\"../data/billboard.csv\")\n",
    "current_billboard = get_billboard(\"../data/billboard2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Yeongcheon/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/Yeongcheon/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/Yeongcheon/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are getting data frame for id = 0\n",
      "The length of dataFrame is 3.\n",
      " \n",
      "We completed 0.00684931506849 of the total work and now are merging the data frame with id = 1\n",
      "The length of dataFrame is 6.\n",
      " \n",
      "We completed 0.013698630137 of the total work and now are merging the data frame with id = 2\n",
      "The length of dataFrame is 9.\n",
      " \n",
      "We completed 0.0205479452055 of the total work and now are merging the data frame with id = 3\n",
      "The length of dataFrame is 12.\n",
      " \n",
      "We completed 0.027397260274 of the total work and now are merging the data frame with id = 4\n",
      "The length of dataFrame is 15.\n",
      " \n",
      "We completed 0.0342465753425 of the total work and now are merging the data frame with id = 5\n",
      "The length of dataFrame is 18.\n",
      " \n",
      "We completed 0.041095890411 of the total work and now are merging the data frame with id = 6\n",
      "The length of dataFrame is 21.\n",
      " \n",
      "We completed 0.0479452054795 of the total work and now are merging the data frame with id = 7\n",
      "The length of dataFrame is 24.\n",
      " \n",
      "We completed 0.0547945205479 of the total work and now are merging the data frame with id = 8\n",
      "The length of dataFrame is 27.\n",
      " \n",
      "We completed 0.0616438356164 of the total work and now are merging the data frame with id = 9\n",
      "The length of dataFrame is 30.\n",
      " \n",
      "We completed 0.0684931506849 of the total work and now are merging the data frame with id = 10\n",
      "The length of dataFrame is 33.\n",
      " \n",
      "We completed 0.0753424657534 of the total work and now are merging the data frame with id = 11\n",
      "The length of dataFrame is 36.\n",
      " \n",
      "We completed 0.0821917808219 of the total work and now are merging the data frame with id = 12\n",
      "The length of dataFrame is 39.\n",
      " \n",
      "We completed 0.0890410958904 of the total work and now are merging the data frame with id = 13\n",
      "The length of dataFrame is 42.\n",
      " \n",
      "We completed 0.0958904109589 of the total work and now are merging the data frame with id = 14\n",
      "The length of dataFrame is 45.\n",
      " \n",
      "We completed 0.102739726027 of the total work and now are merging the data frame with id = 15\n",
      "The length of dataFrame is 48.\n",
      " \n",
      "We completed 0.109589041096 of the total work and now are merging the data frame with id = 16\n",
      "The length of dataFrame is 51.\n",
      " \n",
      "We completed 0.116438356164 of the total work and now are merging the data frame with id = 17\n",
      "The length of dataFrame is 54.\n",
      " \n",
      "We completed 0.123287671233 of the total work and now are merging the data frame with id = 18\n",
      "The length of dataFrame is 57.\n",
      " \n",
      "We completed 0.130136986301 of the total work and now are merging the data frame with id = 19\n",
      "The length of dataFrame is 60.\n",
      " \n",
      "We completed 0.13698630137 of the total work and now are merging the data frame with id = 20\n",
      "The length of dataFrame is 63.\n",
      " \n",
      "We completed 0.143835616438 of the total work and now are merging the data frame with id = 21\n",
      "The length of dataFrame is 66.\n",
      " \n",
      "We completed 0.150684931507 of the total work and now are merging the data frame with id = 22\n",
      "The length of dataFrame is 69.\n",
      " \n",
      "We completed 0.157534246575 of the total work and now are merging the data frame with id = 23\n",
      "The length of dataFrame is 72.\n",
      " \n",
      "We completed 0.164383561644 of the total work and now are merging the data frame with id = 24\n",
      "The length of dataFrame is 75.\n",
      " \n",
      "We completed 0.171232876712 of the total work and now are merging the data frame with id = 25\n",
      "The length of dataFrame is 78.\n",
      " \n",
      "We completed 0.178082191781 of the total work and now are merging the data frame with id = 26\n",
      "The length of dataFrame is 81.\n",
      " \n",
      "We completed 0.184931506849 of the total work and now are merging the data frame with id = 27\n",
      "The length of dataFrame is 84.\n",
      " \n",
      "We completed 0.191780821918 of the total work and now are merging the data frame with id = 28\n",
      "The length of dataFrame is 87.\n",
      " \n",
      "We completed 0.198630136986 of the total work and now are merging the data frame with id = 29\n",
      "The length of dataFrame is 90.\n",
      " \n",
      "We completed 0.205479452055 of the total work and now are merging the data frame with id = 30\n",
      "The length of dataFrame is 93.\n",
      " \n",
      "We completed 0.212328767123 of the total work and now are merging the data frame with id = 31\n",
      "The length of dataFrame is 96.\n",
      " \n",
      "We completed 0.219178082192 of the total work and now are merging the data frame with id = 32\n",
      "The length of dataFrame is 99.\n",
      " \n",
      "We completed 0.22602739726 of the total work and now are merging the data frame with id = 33\n",
      "The length of dataFrame is 102.\n",
      " \n",
      "We completed 0.232876712329 of the total work and now are merging the data frame with id = 34\n",
      "The length of dataFrame is 105.\n",
      " \n",
      "We completed 0.239726027397 of the total work and now are merging the data frame with id = 35\n",
      "The length of dataFrame is 108.\n",
      " \n",
      "We completed 0.246575342466 of the total work and now are merging the data frame with id = 36\n",
      "The length of dataFrame is 111.\n",
      " \n",
      "We completed 0.253424657534 of the total work and now are merging the data frame with id = 37\n",
      "The length of dataFrame is 114.\n",
      " \n",
      "We completed 0.260273972603 of the total work and now are merging the data frame with id = 38\n",
      "The length of dataFrame is 117.\n",
      " \n",
      "We completed 0.267123287671 of the total work and now are merging the data frame with id = 39\n",
      "The length of dataFrame is 120.\n",
      " \n",
      "We completed 0.27397260274 of the total work and now are merging the data frame with id = 40\n",
      "The length of dataFrame is 123.\n",
      " \n",
      "We completed 0.280821917808 of the total work and now are merging the data frame with id = 41\n",
      "The length of dataFrame is 126.\n",
      " \n",
      "We completed 0.287671232877 of the total work and now are merging the data frame with id = 42\n",
      "The length of dataFrame is 129.\n",
      " \n",
      "We completed 0.294520547945 of the total work and now are merging the data frame with id = 43\n",
      "The length of dataFrame is 132.\n",
      " \n",
      "We completed 0.301369863014 of the total work and now are merging the data frame with id = 44\n",
      "The length of dataFrame is 135.\n",
      " \n",
      "We completed 0.308219178082 of the total work and now are merging the data frame with id = 45\n",
      "The length of dataFrame is 138.\n",
      " \n",
      "We completed 0.315068493151 of the total work and now are merging the data frame with id = 46\n",
      "The length of dataFrame is 141.\n",
      " \n",
      "We completed 0.321917808219 of the total work and now are merging the data frame with id = 47\n",
      "The length of dataFrame is 144.\n",
      " \n",
      "We completed 0.328767123288 of the total work and now are merging the data frame with id = 48\n",
      "The length of dataFrame is 147.\n",
      " \n",
      "We completed 0.335616438356 of the total work and now are merging the data frame with id = 49\n",
      "The length of dataFrame is 150.\n",
      " \n",
      "We completed 0.342465753425 of the total work and now are merging the data frame with id = 50\n",
      "The length of dataFrame is 153.\n",
      " \n",
      "We completed 0.349315068493 of the total work and now are merging the data frame with id = 51\n",
      "The length of dataFrame is 156.\n",
      " \n",
      "We completed 0.356164383562 of the total work and now are merging the data frame with id = 52\n",
      "The length of dataFrame is 159.\n",
      " \n",
      "We completed 0.36301369863 of the total work and now are merging the data frame with id = 53\n",
      "The length of dataFrame is 162.\n",
      " \n",
      "We completed 0.369863013699 of the total work and now are merging the data frame with id = 54\n",
      "The length of dataFrame is 165.\n",
      " \n",
      "We completed 0.376712328767 of the total work and now are merging the data frame with id = 55\n",
      "The length of dataFrame is 168.\n",
      " \n",
      "We completed 0.383561643836 of the total work and now are merging the data frame with id = 56\n",
      "The length of dataFrame is 171.\n",
      " \n",
      "We completed 0.390410958904 of the total work and now are merging the data frame with id = 57\n",
      "The length of dataFrame is 174.\n",
      " \n",
      "We completed 0.397260273973 of the total work and now are merging the data frame with id = 58\n",
      "The length of dataFrame is 177.\n",
      " \n",
      "We completed 0.404109589041 of the total work and now are merging the data frame with id = 59\n",
      "The length of dataFrame is 180.\n",
      " \n",
      "We completed 0.41095890411 of the total work and now are merging the data frame with id = 60\n",
      "The length of dataFrame is 183.\n",
      " \n",
      "We completed 0.417808219178 of the total work and now are merging the data frame with id = 61\n",
      "The length of dataFrame is 186.\n",
      " \n",
      "We completed 0.424657534247 of the total work and now are merging the data frame with id = 62\n",
      "The length of dataFrame is 189.\n",
      " \n",
      "We completed 0.431506849315 of the total work and now are merging the data frame with id = 63\n",
      "The length of dataFrame is 192.\n",
      " \n",
      "We completed 0.438356164384 of the total work and now are merging the data frame with id = 64\n",
      "The length of dataFrame is 195.\n",
      " \n",
      "We completed 0.445205479452 of the total work and now are merging the data frame with id = 65\n",
      "The length of dataFrame is 198.\n",
      " \n",
      "We completed 0.452054794521 of the total work and now are merging the data frame with id = 66\n",
      "The length of dataFrame is 201.\n",
      " \n",
      "We completed 0.458904109589 of the total work and now are merging the data frame with id = 67\n",
      "The length of dataFrame is 204.\n",
      " \n",
      "We completed 0.465753424658 of the total work and now are merging the data frame with id = 68\n",
      "The length of dataFrame is 207.\n",
      " \n",
      "We completed 0.472602739726 of the total work and now are merging the data frame with id = 69\n",
      "The length of dataFrame is 210.\n",
      " \n",
      "We completed 0.479452054795 of the total work and now are merging the data frame with id = 70\n",
      "The length of dataFrame is 213.\n",
      " \n",
      "We completed 0.486301369863 of the total work and now are merging the data frame with id = 71\n",
      "The length of dataFrame is 216.\n",
      " \n",
      "We completed 0.493150684932 of the total work and now are merging the data frame with id = 72\n",
      "The length of dataFrame is 219.\n",
      " \n",
      "We completed 0.5 of the total work and now are merging the data frame with id = 73\n",
      "The length of dataFrame is 222.\n",
      " \n",
      "We completed 0.506849315068 of the total work and now are merging the data frame with id = 74\n",
      "The length of dataFrame is 225.\n",
      " \n",
      "We completed 0.513698630137 of the total work and now are merging the data frame with id = 75\n",
      "The length of dataFrame is 228.\n",
      " \n",
      "We completed 0.520547945205 of the total work and now are merging the data frame with id = 76\n",
      "The length of dataFrame is 231.\n",
      " \n",
      "We completed 0.527397260274 of the total work and now are merging the data frame with id = 77\n",
      "The length of dataFrame is 234.\n",
      " \n",
      "We completed 0.534246575342 of the total work and now are merging the data frame with id = 78\n",
      "The length of dataFrame is 237.\n",
      " \n",
      "We completed 0.541095890411 of the total work and now are merging the data frame with id = 79\n",
      "The length of dataFrame is 240.\n",
      " \n",
      "We completed 0.547945205479 of the total work and now are merging the data frame with id = 80\n",
      "The length of dataFrame is 243.\n",
      " \n",
      "We completed 0.554794520548 of the total work and now are merging the data frame with id = 81\n",
      "The length of dataFrame is 246.\n",
      " \n",
      "We completed 0.561643835616 of the total work and now are merging the data frame with id = 82\n",
      "The length of dataFrame is 249.\n",
      " \n",
      "We completed 0.568493150685 of the total work and now are merging the data frame with id = 83\n",
      "The length of dataFrame is 252.\n",
      " \n",
      "We completed 0.575342465753 of the total work and now are merging the data frame with id = 84\n",
      "The length of dataFrame is 255.\n",
      " \n",
      "We completed 0.582191780822 of the total work and now are merging the data frame with id = 85\n",
      "The length of dataFrame is 258.\n",
      " \n",
      "We completed 0.58904109589 of the total work and now are merging the data frame with id = 86\n",
      "The length of dataFrame is 261.\n",
      " \n",
      "We completed 0.595890410959 of the total work and now are merging the data frame with id = 87\n",
      "The length of dataFrame is 264.\n",
      " \n",
      "We completed 0.602739726027 of the total work and now are merging the data frame with id = 88\n",
      "The length of dataFrame is 267.\n",
      " \n",
      "We completed 0.609589041096 of the total work and now are merging the data frame with id = 89\n",
      "The length of dataFrame is 270.\n",
      " \n",
      "We completed 0.616438356164 of the total work and now are merging the data frame with id = 90\n",
      "The length of dataFrame is 273.\n",
      " \n",
      "We completed 0.623287671233 of the total work and now are merging the data frame with id = 91\n",
      "The length of dataFrame is 276.\n",
      " \n",
      "We completed 0.630136986301 of the total work and now are merging the data frame with id = 92\n",
      "The length of dataFrame is 279.\n",
      " \n",
      "We completed 0.63698630137 of the total work and now are merging the data frame with id = 93\n",
      "The length of dataFrame is 282.\n",
      " \n",
      "We completed 0.643835616438 of the total work and now are merging the data frame with id = 94\n",
      "The length of dataFrame is 285.\n",
      " \n",
      "We completed 0.650684931507 of the total work and now are merging the data frame with id = 95\n",
      "The length of dataFrame is 288.\n",
      " \n",
      "We completed 0.657534246575 of the total work and now are merging the data frame with id = 96\n",
      "The length of dataFrame is 291.\n",
      " \n",
      "We completed 0.664383561644 of the total work and now are merging the data frame with id = 97\n",
      "The length of dataFrame is 294.\n",
      " \n",
      "We completed 0.671232876712 of the total work and now are merging the data frame with id = 98\n",
      "The length of dataFrame is 297.\n",
      " \n",
      "We completed 0.678082191781 of the total work and now are merging the data frame with id = 99\n",
      "The length of dataFrame is 300.\n",
      " \n",
      "We completed 0.684931506849 of the total work and now are merging the data frame with id = 100\n",
      "The length of dataFrame is 303.\n",
      " \n",
      "We completed 0.691780821918 of the total work and now are merging the data frame with id = 101\n",
      "The length of dataFrame is 306.\n",
      " \n",
      "We completed 0.698630136986 of the total work and now are merging the data frame with id = 102\n",
      "The length of dataFrame is 309.\n",
      " \n",
      "We completed 0.705479452055 of the total work and now are merging the data frame with id = 103\n",
      "The length of dataFrame is 312.\n",
      " \n",
      "We completed 0.712328767123 of the total work and now are merging the data frame with id = 104\n",
      "The length of dataFrame is 315.\n",
      " \n",
      "We completed 0.719178082192 of the total work and now are merging the data frame with id = 105\n",
      "The length of dataFrame is 318.\n",
      " \n",
      "We completed 0.72602739726 of the total work and now are merging the data frame with id = 106\n",
      "The length of dataFrame is 321.\n",
      " \n",
      "We completed 0.732876712329 of the total work and now are merging the data frame with id = 107\n",
      "The length of dataFrame is 324.\n",
      " \n",
      "We completed 0.739726027397 of the total work and now are merging the data frame with id = 108\n",
      "The length of dataFrame is 327.\n",
      " \n",
      "We completed 0.746575342466 of the total work and now are merging the data frame with id = 109\n",
      "The length of dataFrame is 330.\n",
      " \n",
      "We completed 0.753424657534 of the total work and now are merging the data frame with id = 110\n",
      "The length of dataFrame is 333.\n",
      " \n",
      "We completed 0.760273972603 of the total work and now are merging the data frame with id = 111\n",
      "The length of dataFrame is 336.\n",
      " \n",
      "We completed 0.767123287671 of the total work and now are merging the data frame with id = 112\n",
      "The length of dataFrame is 339.\n",
      " \n",
      "We completed 0.77397260274 of the total work and now are merging the data frame with id = 113\n",
      "The length of dataFrame is 342.\n",
      " \n",
      "We completed 0.780821917808 of the total work and now are merging the data frame with id = 114\n",
      "The length of dataFrame is 345.\n",
      " \n",
      "We completed 0.787671232877 of the total work and now are merging the data frame with id = 115\n",
      "The length of dataFrame is 348.\n",
      " \n",
      "We completed 0.794520547945 of the total work and now are merging the data frame with id = 116\n",
      "The length of dataFrame is 351.\n",
      " \n",
      "We completed 0.801369863014 of the total work and now are merging the data frame with id = 117\n",
      "The length of dataFrame is 354.\n",
      " \n",
      "We completed 0.808219178082 of the total work and now are merging the data frame with id = 118\n",
      "The length of dataFrame is 357.\n",
      " \n",
      "We completed 0.815068493151 of the total work and now are merging the data frame with id = 119\n",
      "The length of dataFrame is 360.\n",
      " \n",
      "We completed 0.821917808219 of the total work and now are merging the data frame with id = 120\n",
      "The length of dataFrame is 363.\n",
      " \n",
      "We completed 0.828767123288 of the total work and now are merging the data frame with id = 121\n",
      "The length of dataFrame is 366.\n",
      " \n",
      "We completed 0.835616438356 of the total work and now are merging the data frame with id = 122\n",
      "The length of dataFrame is 369.\n",
      " \n",
      "We completed 0.842465753425 of the total work and now are merging the data frame with id = 123\n",
      "The length of dataFrame is 372.\n",
      " \n",
      "We completed 0.849315068493 of the total work and now are merging the data frame with id = 124\n",
      "The length of dataFrame is 375.\n",
      " \n",
      "We completed 0.856164383562 of the total work and now are merging the data frame with id = 125\n",
      "The length of dataFrame is 378.\n",
      " \n",
      "We completed 0.86301369863 of the total work and now are merging the data frame with id = 126\n",
      "The length of dataFrame is 381.\n",
      " \n",
      "We completed 0.869863013699 of the total work and now are merging the data frame with id = 127\n",
      "The length of dataFrame is 384.\n",
      " \n",
      "We completed 0.876712328767 of the total work and now are merging the data frame with id = 128\n",
      "The length of dataFrame is 387.\n",
      " \n",
      "We completed 0.883561643836 of the total work and now are merging the data frame with id = 129\n",
      "The length of dataFrame is 390.\n",
      " \n",
      "We completed 0.890410958904 of the total work and now are merging the data frame with id = 130\n",
      "The length of dataFrame is 393.\n",
      " \n",
      "We completed 0.897260273973 of the total work and now are merging the data frame with id = 131\n",
      "The length of dataFrame is 396.\n",
      " \n",
      "We completed 0.904109589041 of the total work and now are merging the data frame with id = 132\n",
      "The length of dataFrame is 399.\n",
      " \n",
      "We completed 0.91095890411 of the total work and now are merging the data frame with id = 133\n",
      "The length of dataFrame is 402.\n",
      " \n",
      "We completed 0.917808219178 of the total work and now are merging the data frame with id = 134\n",
      "The length of dataFrame is 405.\n",
      " \n",
      "We completed 0.924657534247 of the total work and now are merging the data frame with id = 135\n",
      "The length of dataFrame is 408.\n",
      " \n",
      "We completed 0.931506849315 of the total work and now are merging the data frame with id = 136\n",
      "The length of dataFrame is 411.\n",
      " \n",
      "We completed 0.938356164384 of the total work and now are merging the data frame with id = 137\n",
      "The length of dataFrame is 414.\n",
      " \n",
      "We completed 0.945205479452 of the total work and now are merging the data frame with id = 138\n",
      "The length of dataFrame is 417.\n",
      " \n",
      "We completed 0.952054794521 of the total work and now are merging the data frame with id = 139\n",
      "The length of dataFrame is 420.\n",
      " \n",
      "We completed 0.958904109589 of the total work and now are merging the data frame with id = 140\n",
      "The length of dataFrame is 423.\n",
      " \n",
      "We completed 0.965753424658 of the total work and now are merging the data frame with id = 141\n",
      "The length of dataFrame is 426.\n",
      " \n",
      "We completed 0.972602739726 of the total work and now are merging the data frame with id = 142\n",
      "The length of dataFrame is 429.\n",
      " \n",
      "We completed 0.979452054795 of the total work and now are merging the data frame with id = 143\n",
      "The length of dataFrame is 432.\n",
      " \n",
      "We completed 0.986301369863 of the total work and now are merging the data frame with id = 144\n",
      "The length of dataFrame is 435.\n",
      " \n",
      "We completed 0.993150684932 of the total work and now are merging the data frame with id = 145\n",
      "The length of dataFrame is 438.\n",
      " \n",
      "We're done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Yeongcheon/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "list_id = range(len(list_input))\n",
    "week_14_16 = [14,15,16]\n",
    "file_name = 'result_06.csv' \n",
    "df_total = merge_dataframe(list_id, db, list_input, current_billboard, file_name, week_all=week_14_16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db = client['song3_database']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "song_id = 0\n",
    "table_name = \"test_01_\" + str(song_id)\n",
    "tab = db[table_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "table = get_dataframe(tab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>created_at</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>source</th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_screen_name</th>\n",
       "      <th>user_name</th>\n",
       "      <th>user_created_at</th>\n",
       "      <th>user_description</th>\n",
       "      <th>user_followers_count</th>\n",
       "      <th>user_friends_count</th>\n",
       "      <th>user_location</th>\n",
       "      <th>user_time_zone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>722086835183292416</td>\n",
       "      <td>@taigassi_twt Lil Dicky – $ave Dat Money</td>\n",
       "      <td>Mon Apr 18 15:38:11 +0000 2016</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;a href=\"http://twitter.com\" rel=\"nofollow\"&gt;Tw...</td>\n",
       "      <td>2602168940</td>\n",
       "      <td>melkiaaj</td>\n",
       "      <td>макнэша'25</td>\n",
       "      <td>Thu Jul 03 19:52:46 +0000 2014</td>\n",
       "      <td>just friends</td>\n",
       "      <td>1050</td>\n",
       "      <td>46</td>\n",
       "      <td>United States</td>\n",
       "      <td>Nairobi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>722077136442519552</td>\n",
       "      <td>#NowPlaying  Lil Dicky - Fetty Wap - Rich Homi...</td>\n",
       "      <td>Mon Apr 18 14:59:39 +0000 2016</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;a href=\"http://share.radionomy.com\" rel=\"nofo...</td>\n",
       "      <td>3744306616</td>\n",
       "      <td>BRMBRadioMusic</td>\n",
       "      <td>BRMB Radio</td>\n",
       "      <td>Wed Sep 23 00:57:27 +0000 2015</td>\n",
       "      <td>Atlanta's #1 fastest growing international rad...</td>\n",
       "      <td>301</td>\n",
       "      <td>197</td>\n",
       "      <td>Atlanta, GA</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>722060914376196096</td>\n",
       "      <td>I liked a @YouTube video https://t.co/XRijPfYy...</td>\n",
       "      <td>Mon Apr 18 13:55:11 +0000 2016</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;a href=\"http://www.google.com/\" rel=\"nofollow...</td>\n",
       "      <td>2299499632</td>\n",
       "      <td>HandsomeHanz</td>\n",
       "      <td>HandsomeHanz</td>\n",
       "      <td>Thu Jan 23 15:43:29 +0000 2014</td>\n",
       "      <td>Period.</td>\n",
       "      <td>132</td>\n",
       "      <td>146</td>\n",
       "      <td>London</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>722058461408849921</td>\n",
       "      <td>I liked a @YouTube video https://t.co/xKfHoBoR...</td>\n",
       "      <td>Mon Apr 18 13:45:27 +0000 2016</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;a href=\"http://www.google.com/\" rel=\"nofollow...</td>\n",
       "      <td>97059953</td>\n",
       "      <td>OfficialGrizzly</td>\n",
       "      <td>✯ ProGrizzly ✯</td>\n",
       "      <td>Tue Dec 15 20:53:54 +0000 2009</td>\n",
       "      <td>22. Entertainer/Entrepreneur #GrizzlyArmy \\\\ O...</td>\n",
       "      <td>1978</td>\n",
       "      <td>270</td>\n",
       "      <td>Michigan, USA</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>722058346174550016</td>\n",
       "      <td>Lil Dicky - $ave Dat Money feat. Fetty Wap and...</td>\n",
       "      <td>Mon Apr 18 13:44:59 +0000 2016</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;a href=\"http://twitter.com\" rel=\"nofollow\"&gt;Tw...</td>\n",
       "      <td>176175086</td>\n",
       "      <td>jamied_uk</td>\n",
       "      <td>Jay Mee</td>\n",
       "      <td>Sun Aug 08 20:01:05 +0000 2010</td>\n",
       "      <td>Please check out my site \\r\\nForum http://t.co...</td>\n",
       "      <td>497</td>\n",
       "      <td>1114</td>\n",
       "      <td>ENGLAND</td>\n",
       "      <td>London</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id                                               text  \\\n",
       "0  722086835183292416           @taigassi_twt Lil Dicky – $ave Dat Money   \n",
       "1  722077136442519552  #NowPlaying  Lil Dicky - Fetty Wap - Rich Homi...   \n",
       "2  722060914376196096  I liked a @YouTube video https://t.co/XRijPfYy...   \n",
       "3  722058461408849921  I liked a @YouTube video https://t.co/xKfHoBoR...   \n",
       "4  722058346174550016  Lil Dicky - $ave Dat Money feat. Fetty Wap and...   \n",
       "\n",
       "                       created_at  favorite_count  \\\n",
       "0  Mon Apr 18 15:38:11 +0000 2016               0   \n",
       "1  Mon Apr 18 14:59:39 +0000 2016               0   \n",
       "2  Mon Apr 18 13:55:11 +0000 2016               0   \n",
       "3  Mon Apr 18 13:45:27 +0000 2016               0   \n",
       "4  Mon Apr 18 13:44:59 +0000 2016               0   \n",
       "\n",
       "                                              source     user_id  \\\n",
       "0  <a href=\"http://twitter.com\" rel=\"nofollow\">Tw...  2602168940   \n",
       "1  <a href=\"http://share.radionomy.com\" rel=\"nofo...  3744306616   \n",
       "2  <a href=\"http://www.google.com/\" rel=\"nofollow...  2299499632   \n",
       "3  <a href=\"http://www.google.com/\" rel=\"nofollow...    97059953   \n",
       "4  <a href=\"http://twitter.com\" rel=\"nofollow\">Tw...   176175086   \n",
       "\n",
       "  user_screen_name       user_name                 user_created_at  \\\n",
       "0         melkiaaj      макнэша'25  Thu Jul 03 19:52:46 +0000 2014   \n",
       "1   BRMBRadioMusic      BRMB Radio  Wed Sep 23 00:57:27 +0000 2015   \n",
       "2     HandsomeHanz    HandsomeHanz  Thu Jan 23 15:43:29 +0000 2014   \n",
       "3  OfficialGrizzly  ✯ ProGrizzly ✯  Tue Dec 15 20:53:54 +0000 2009   \n",
       "4        jamied_uk         Jay Mee  Sun Aug 08 20:01:05 +0000 2010   \n",
       "\n",
       "                                    user_description  user_followers_count  \\\n",
       "0                             　　　　　　　　　　just friends                  1050   \n",
       "1  Atlanta's #1 fastest growing international rad...                   301   \n",
       "2                                            Period.                   132   \n",
       "3  22. Entertainer/Entrepreneur #GrizzlyArmy \\\\ O...                  1978   \n",
       "4  Please check out my site \\r\\nForum http://t.co...                   497   \n",
       "\n",
       "   user_friends_count  user_location              user_time_zone  \n",
       "0                  46  United States                     Nairobi  \n",
       "1                 197    Atlanta, GA                        None  \n",
       "2                 146        London                         None  \n",
       "3                 270  Michigan, USA  Eastern Time (US & Canada)  \n",
       "4                1114        ENGLAND                      London  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0               @taigassi_twt Lil Dicky – $ave Dat Money\n",
       "1      #NowPlaying  Lil Dicky - Fetty Wap - Rich Homi...\n",
       "2      I liked a @YouTube video https://t.co/XRijPfYy...\n",
       "3      I liked a @YouTube video https://t.co/xKfHoBoR...\n",
       "4      Lil Dicky - $ave Dat Money feat. Fetty Wap and...\n",
       "5      *jangan hedon* ♫ $ave Dat Money (feat. Fetty W...\n",
       "6      RT @12riku011: Elle Teresaの世界はマネーの原曲🤑\\n\\nList...\n",
       "7      They were just talking about lil dicky on the ...\n",
       "8      RT @12riku011: Elle Teresaの世界はマネーの原曲🤑\\n\\nList...\n",
       "9      I added a video to a @YouTube playlist https:/...\n",
       "10     Now Playing on Urban Hitz Radio: Lil' Dicky - ...\n",
       "11     LIL DICKY F. FETTY WAP AND RICH HOMIE QUAN - $...\n",
       "12     #SF ---&gt; #HNL ---&gt; #Guam ---&gt; #HNL --...\n",
       "13     #NowPlaying $ave Dat Money ft Fetty Wap &amp; ...\n",
       "14     I liked a @YouTube video https://t.co/PTRXXoF3...\n",
       "15     Lil Dicky - $ave Dat Money feat. Fetty Wap and...\n",
       "16     $ave Dat Money by Lil Dicky Ft. Fetty Wap &amp...\n",
       "17     RT @HIPHOPTRlBE: Lil Dicky feat. Fetty Wap &am...\n",
       "18     I liked a @YouTube video https://t.co/M2Ga1tRu...\n",
       "19     so if I rap lil dicky and rich homies part of ...\n",
       "20     #NowPlaying  Lil Dicky - Fetty Wap - Rich Homi...\n",
       "21     #NowPlaying $ave Dat Money (feat. Fetty Wap &a...\n",
       "22     I added a video to a @YouTube playlist https:/...\n",
       "23     I liked a @YouTube video https://t.co/9ZS1TCTu...\n",
       "24     I liked a @YouTube video https://t.co/vRS1GlWL...\n",
       "25     I liked a @YouTube video https://t.co/JClhpxbq...\n",
       "26     I liked a @YouTube video https://t.co/XYVhErlA...\n",
       "27     Now Playing on Urban Hitz Radio: Lil' Dicky - ...\n",
       "28     I liked a @YouTube video https://t.co/N9wKtvNY...\n",
       "29     Lil Dicky - $ave Dat Money feat. Fetty Wap and...\n",
       "                             ...                        \n",
       "345    TOUT DE SUITE  je vous diffuse :\\nLil Dicky - ...\n",
       "346    RT @HIPHOPTRlBE: Lil Dicky feat. Fetty Wap &am...\n",
       "381    #NowPlaying Lil Dicky - $ave Dat Money Feat Fe...\n",
       "401    I liked a @YouTube video https://t.co/IfsDezfx...\n",
       "465    $ave Dat Money - Lil Dicky feat. Fetty Wap and...\n",
       "479    #NowPlaying Lil Dicky - $ave Dat Money Feat Fe...\n",
       "482    Gotta Save it 🤑🤑🤑🤑\\n\\n$ave Dat Money (Feat...\n",
       "515    #NowPlaying Lil Dicky - $ave Dat Money Feat Fe...\n",
       "531    I liked a @YouTube video https://t.co/yv1SRKmI...\n",
       "546    #NowPlaying $ave Dat Money (feat. Fetty Wap &a...\n",
       "551    #NowPlaying Lil Dicky - $ave Dat Money Feat Fe...\n",
       "570    RT @HIPHOPTRlBE: Lil Dicky feat. Fetty Wap &am...\n",
       "589    #NowPlaying Lil Dicky - $ave Dat Money Feat Fe...\n",
       "590    Gostei de um vídeo do @YouTube https://t.co/z4...\n",
       "602    Lil Dicky feat. Fetty Wap and Rich Homie Quan ...\n",
       "606    Lil Dicky – $ave Dat Money feat. Fetty Wap and...\n",
       "607    Lil Dicky – $ave Dat Money feat. Fetty Wap and...\n",
       "608    Lil Dicky – $ave Dat Money feat. Fetty Wap and...\n",
       "609    Lil Dicky – $ave Dat Money feat. Fetty Wap and...\n",
       "610    Lil Dicky – $ave Dat Money feat. Fetty Wap and...\n",
       "613    Lil Dicky – $ave Dat Money ft. Fetty Wap and R...\n",
       "618    I liked a @YouTube video https://t.co/2OKpHVsl...\n",
       "631    RT @Cierra_Mist_: Lil Dicky - $ave Dat Money f...\n",
       "646    #NowPlaying Lil Dicky - $ave Dat Money Feat Fe...\n",
       "651    Yeah! Our New Anthem Of Saving Money! Lil Dick...\n",
       "659    Lil Dicky – $ave Dat Money feat. Fet... https:...\n",
       "662    ★ Lil Dicky – $ave Dat Money feat. Fetty Wap a...\n",
       "663    Lil Dicky – $ave Dat Money feat. Fetty Wap and...\n",
       "664    Lil Dicky – $ave Dat Money feat. Fetty Wap and...\n",
       "682    RT @HIPHOPTRlBE: Lil Dicky feat. Fetty Wap &am...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'#NowPlaying $ave Dat Money (feat. Fetty Wap &amp; Rich Homie Quan) by Lil Dicky \\u266b https://t.co/jV7Fnr3e2F'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table['text'][21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'I liked a @YouTube video https://t.co/yv1SRKmI3N Lil Dicky - $ave Dat Money feat. Fetty Wap and Rich Homie Quan (Official Music Video)'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table['text'][531]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
